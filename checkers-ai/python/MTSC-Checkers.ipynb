{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b6c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36945892",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2299b",
   "metadata": {},
   "source": [
    "Environment closely follows OpenAI gym API. Currently can not be invoked with ```gym.make(\"env_id\")```, though it should be easy to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f4ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env\n",
    "\n",
    "env = Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125c716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monte_carlo_tree\n",
    "\n",
    "def state_to_board(state):\n",
    "    board = np.zeros((5, 8, 8))\n",
    "    for piece in state['pieces']:\n",
    "        if piece['color'] == \"Black\":\n",
    "            board[0, piece['x'], piece['y']] = 1\n",
    "        else: \n",
    "            board[1, piece['x'], piece['y']] = 1\n",
    "        board[2] = 1 if state['turn']['color'] == \"Black\" else 0\n",
    "\n",
    "    return board\n",
    "\n",
    "\n",
    "class CheckersNode(monte_carlo_tree.Node):\n",
    "    \"\"\"Go Game Tree Node\"\"\"\n",
    "\n",
    "    def whites(self):\n",
    "        \"\"\"White pieces on board\"\"\"\n",
    "        return state_to_board(self.state)[1]\n",
    "    \n",
    "    def blacks(self):\n",
    "        \"\"\"Black pieces on board\"\"\"\n",
    "        return state_to_board(self.state)[0]\n",
    "\n",
    "    def get_piece_by_id(self, piece_id):\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda p: p['id'] == piece_id, self.state['pieces']\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if len(pieces) > 0:\n",
    "            return pieces[0]\n",
    "\n",
    "        return next(\n",
    "            filter(lambda p: p['id'] == piece_id, \n",
    "                self.state['pieces']\n",
    "            ))\n",
    "\n",
    "    def get_piece_by_coord(self, x, y):\n",
    "        return next(\n",
    "            filter(lambda p: p['x'] == x and p['y'] == y, \n",
    "                self.state['pieces']\n",
    "            ))\n",
    "\n",
    "    def current_player(self):\n",
    "        \"\"\"Return 1 if current player plays black, and -1 for whites\"\"\"\n",
    "        if self.state['turn']['color'] == 'Black':\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def possible_actions(self, player=None):\n",
    "        \"\"\"List of possible next actions\"\"\"\n",
    "        if player == None:\n",
    "            player = self.current_player()\n",
    "\n",
    "        coords = []\n",
    "        for piece_id in range(0, 24):\n",
    "            piece = self.get_piece_by_id(piece_id)\n",
    "            \n",
    "            for action in self.state['moveset'][piece_id]:\n",
    "                coords.append((piece['x'], piece['y'], action[0], action[1]))\n",
    "\n",
    "        actions = np.zeros((8, 8, 8, 8))\n",
    "        for c in coords:\n",
    "            actions[c] = 1\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def possible_actions_list(self):\n",
    "        actions = self.possible_actions()\n",
    "        return np.argwhere(actions==1)\n",
    "\n",
    "    def prepare_action(self, action):\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, env):\n",
    "        return self.reward\n",
    "\n",
    "    def prepare_action(self, action):\n",
    "        pa = {\n",
    "            'piece': self.get_piece_by_coord(action[0], action[1]),\n",
    "            'square': {'x': int(action[2]), 'y': int(action[3])}\n",
    "        }\n",
    "        return pa\n",
    "\n",
    "    def prepare_state(self, player=None):\n",
    "        \"\"\"\n",
    "        Prepare game state X from perspective of current player\n",
    "        [\n",
    "            [ 1 -1 -1 ]\n",
    "            [ 1  0  0 ]\n",
    "            [ 0  0 -1 ]\n",
    "        ]\n",
    "\n",
    "        \n",
    "        Where  \n",
    "            1:  current player\n",
    "            -1: opposing player\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        if player == None:\n",
    "            player = self.current_player()\n",
    "\n",
    "        # take advantage of game symmetry        \n",
    "        state = self.blacks() - self.whites() if player == 1 else self.whites() - self.blacks()\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a7605",
   "metadata": {},
   "source": [
    "## Random Play Tree\n",
    "\n",
    "This is very basic algorithm that plays the game by making random moves. Sometimes it reaches the end goal, but overall it supper inneficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b968af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = monte_carlo_tree.RandomPlayTree(env, CheckersNode, BOARD_SIZE)\n",
    "\n",
    "\n",
    "def random_play():\n",
    "    '''\n",
    "    Play a game using random tree strategy\n",
    "    '''\n",
    "    return rt.simulate(rt.root_node)\n",
    "    \n",
    "\n",
    "def build_stats(playfunc, n_games=100):\n",
    "    '''\n",
    "    Play a number of random games and display result\n",
    "    '''\n",
    "\n",
    "    black_wins = 0\n",
    "    white_wins = 0\n",
    "    draws = 0\n",
    "    moves = []\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        m = playfunc()\n",
    "\n",
    "        reward = m.reward\n",
    "\n",
    "        if reward > 0:\n",
    "            black_wins += 1\n",
    "        elif reward < 0:\n",
    "            white_wins += 1\n",
    "        elif reward == 0.0:\n",
    "            draws += 1\n",
    "       \n",
    "        moves.append(m.depth())\n",
    "    \n",
    "    print(\"Blacks: \", black_wins, \"Whites: \", white_wins, \"Draws: \", draws, \"Moves mean:\", np.mean(moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9e4494",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1160\\3792525695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbuild_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_play\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1160\\2405453572.py\u001b[0m in \u001b[0;36mbuild_stats\u001b[1;34m(playfunc, n_games)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_games\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1160\\2405453572.py\u001b[0m in \u001b[0;36mrandom_play\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mPlay\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgame\u001b[0m \u001b[0musing\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0mtree\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     '''\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sergei\\git\\checkers\\checkers-ai\\python\\monte_carlo_tree.py\u001b[0m in \u001b[0;36msimulate\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mcurrent_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sergei\\git\\checkers\\checkers-ai\\python\\monte_carlo_tree.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, node, action, prob)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mpossible_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpossible_actions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1160\\583994524.py\u001b[0m in \u001b[0;36mpossible_actions_list\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpossible_actions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1160\\583994524.py\u001b[0m in \u001b[0;36mpossible_actions\u001b[1;34m(self, player)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpiece_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mpiece\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_piece_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiece_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'moveset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpiece_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1160\\583994524.py\u001b[0m in \u001b[0;36mget_piece_by_id\u001b[1;34m(self, piece_id)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_piece_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpiece_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         return next(\n\u001b[0m\u001b[0;32m     28\u001b[0m             filter(lambda p: p['id'] == piece_id, \n\u001b[0;32m     29\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pieces'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "build_stats(random_play, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee835d",
   "metadata": {},
   "source": [
    "## Monte Carlo Search Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9965a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcst = monte_carlo_tree.MonteCarloPlayTree(env, CheckersNode, BOARD_SIZE)\n",
    "\n",
    "def mtsc_play():\n",
    "    '''\n",
    "    Play a game using MonteCarloSearchTree\n",
    "    '''\n",
    "    \n",
    "    return mcst.simulate(mcst.root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeae88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_stats(mtsc_play, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246da071",
   "metadata": {},
   "source": [
    "## Alpha Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438b18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, board_size=BOARD_SIZE):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.board_size = board_size\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 8192, kernel_size=3, padding=1)\n",
    "        self.layer1 = nn.Linear(8192, 4096)\n",
    "        self.layer2 = nn.Linear(8192, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = x.view(-1, 8192)\n",
    "        \n",
    "        prob = F.hardsigmoid(self.layer1(x))\n",
    "        value = F.hardtanh(self.layer2(x))\n",
    "\n",
    "        return prob.view(-1, 8, 8, 8, 8), value.view(-1, 1)\n",
    "\n",
    "actor_critic_network = ActorCritic().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07f5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "azt = monte_carlo_tree.GuidedMonteCarloPlayTree(env, CheckersNode, BOARD_SIZE, actor_critic_network, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "284e757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defa06d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0\n",
      "number of actions:  96\n",
      "Loss: tensor(271.6643, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 1\n",
      "number of actions:  82\n",
      "Loss: tensor(312.7006, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 2\n",
      "number of actions:  90\n",
      "Loss: tensor(167.1609, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 3\n",
      "number of actions:  98\n",
      "Loss: tensor(276.8374, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 4\n",
      "number of actions:  96\n",
      "Loss: tensor(365.6924, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 5\n",
      "number of actions:  81\n",
      "Loss: tensor(330.5529, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 6\n",
      "number of actions:  91\n",
      "Loss: tensor(340.1009, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 7\n",
      "number of actions:  101\n",
      "Loss: tensor(317.9117, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 8\n",
      "number of actions:  96\n",
      "Loss: tensor(239.1516, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 9\n",
      "number of actions:  68\n",
      "Loss: tensor(198.1733, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 10\n",
      "number of actions:  98\n",
      "Loss: tensor(270.4628, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 11\n",
      "number of actions:  96\n",
      "Loss: tensor(204.9608, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 12\n",
      "number of actions:  99\n",
      "Loss: tensor(209.7095, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 13\n",
      "number of actions:  96\n",
      "Loss: tensor(175.6624, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 14\n",
      "number of actions:  95\n",
      "Loss: tensor(149.5844, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 15\n",
      "number of actions:  33\n",
      "Loss: tensor(45.3362, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 16\n",
      "number of actions:  99\n",
      "Loss: tensor(190.2111, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 17\n",
      "number of actions:  95\n",
      "Loss: tensor(118.5537, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 18\n",
      "number of actions:  97\n",
      "Loss: tensor(118.6307, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 19\n",
      "number of actions:  97\n",
      "Loss: tensor(104.0949, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 20\n",
      "number of actions:  97\n",
      "Loss: tensor(84.1549, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 21\n",
      "number of actions:  96\n",
      "Loss: tensor(42.1551, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 22\n",
      "number of actions:  91\n",
      "Loss: tensor(397.5246, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 23\n",
      "number of actions:  102\n",
      "Loss: tensor(17.9164, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 24\n",
      "number of actions:  96\n",
      "Loss: tensor(12.4466, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 25\n",
      "number of actions:  105\n",
      "Loss: tensor(36.2371, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 26\n",
      "number of actions:  90\n",
      "Loss: tensor(2.5537, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 27\n",
      "number of actions:  92\n",
      "Loss: tensor(1.4162, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 28\n",
      "number of actions:  94\n",
      "Loss: tensor(8.4043, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 29\n",
      "number of actions:  95\n",
      "Loss: tensor(8.5825, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 30\n",
      "number of actions:  85\n",
      "Loss: tensor(0.2108, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 31\n",
      "number of actions:  96\n",
      "Loss: tensor(1.1906, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 32\n",
      "number of actions:  105\n",
      "Loss: tensor(11.8791, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 33\n",
      "number of actions:  93\n",
      "Loss: tensor(0.8398, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 34\n",
      "number of actions:  90\n",
      "Loss: tensor(0.7769, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 35\n",
      "number of actions:  90\n",
      "Loss: tensor(0.9701, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 36\n",
      "number of actions:  88\n",
      "Loss: tensor(1.4199, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 37\n",
      "number of actions:  97\n",
      "Loss: tensor(4.7696, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 38\n",
      "number of actions:  94\n",
      "Loss: tensor(-0.0027, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 39\n",
      "number of actions:  36\n",
      "Loss: tensor(144.1472, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 40\n",
      "number of actions:  101\n",
      "Loss: tensor(1.4396, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 41\n",
      "number of actions:  107\n",
      "Loss: tensor(9.5243, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 42\n",
      "number of actions:  103\n",
      "Loss: tensor(4.3512, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration # 43\n"
     ]
    }
   ],
   "source": [
    "for loss in azt.train(200):\n",
    "    losses.append(loss)\n",
    "torch.save(actor_critic_network.state_dict(), \"./checkers.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "plt.plot([l.detach().cpu().numpy() for l in losses])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
